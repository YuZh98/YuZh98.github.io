[{"categories":null,"content":"This contains my notes for Theory of Linear Models","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Instructor Prof. James Hobert Taken by Yu Zheng ","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:0:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Review of Linear Algebra Def: A set of vectors $\\mathbf x^{(1)}, \\mathbf x^{(2)},\\cdots,\\mathbf x^{(n)}$ is linearly dependent if there exist coefficients $c_j$, $j=1,\\cdots,n$, not all zero, such that $$ \\sum_{j=1}^nc_j\\mathbf x^{(j)}=\\mathbf 0. $$ This set of vectors is linearly independent if $\\sum_{j=1}^nc_j\\mathbf x^{(j)}=\\mathbf 0$ implies $c_j\\equiv0$. Def: Two vectors are orthogonal to each other, written $\\mathbf x\\perp \\mathbf y$, if $$ \\mathbf x^T\\mathbf y=\\mathbf y^T\\mathbf x=\\sum_j x_jy_j=0. $$ A set of vectors $\\mathbf x^{(1)}, \\mathbf x^{(2)}, \\cdots, \\mathbf x^{(n)}$ are mutually orthogonal iff $\\mathbf x^{(j)T}\\mathbf x^{(j)}=0$ for all $i\\not=j$. A set of mutually orthogonal nonzero vectors are also linearly independent. Def: A vector space $\\mathcal S$ is a set of vectors that are closed under addition and scalar multiplication, that is, if $\\mathbf x^{(1)}$ and $\\mathbf x^{(2)}$ are in $\\mathcal S$, then $c_1\\mathbf x^{(1)}+c_2\\mathbf x^{(2)}$ is in $\\mathcal S$. Def: A vector space $\\mathcal S$ is said to be generated by a set of vectors $\\mathbf x ^{(1)}, \\mathbf x^{(2)},\\cdots, \\mathbf x^{(n)}$ if for every $\\mathbf x\\in\\mathcal S$, there exist some coefficients $c_j$ so that we can write $$ \\mathbf x=\\sum_{j}c_j\\mathbf x^{(j)}. $$ Def: If a vector space $\\mathcal S$ is generated by a set of linearly independent vectors $\\mathbf x ^{(1)}, \\mathbf x^{(2)},\\cdots, \\mathbf x^{(n)}$, then this set of vectors form a basis for the space $\\mathcal S$. Def: The set of all linear combinations of a set of vectors is called the span of that set. $\\mathbf x\\in span\\{\\mathbf x ^{(1)}, \\mathbf x^{(2)},\\cdots, \\mathbf x^{(n)}\\}$ if and only if there exists constants $c_j$ such that $\\mathbf x=\\sum_jc_j\\mathbf x^{(j)}$. Def: The number of vectors in the basis for a vector space $\\mathcal S$ is the dimension of the space $\\mathcal S$, written $dim(\\mathcal S)$. Def: The rank of a matrix $\\mathbf A$ is the number of linear independent rows or columns, and denoted by $rank(\\mathbf A)$ or $r(\\mathbf A)$. Def: The column space of a matrix, denoted by $\\mathcal C(\\mathbf A)$, is the vector space spannned by the columns of the matrix, that is $$ \\mathcal C(\\mathbf A )=\\{\\mathbf x: \\text{there exists a vectors }\\mathbf c\\text{ such that }\\mathbf x=\\mathbf A \\mathbf c \\}. $$ $\\dim(\\mathcal C(\\mathbf A))=rank(\\mathbf A)$ $rank(\\mathbf A \\mathbf B)\\leq \\min(rank(\\mathbf A),rank(\\mathbf B))$ $\\mathcal C(\\mathbf A\\mathbf B)\\subset \\mathcal C(\\mathbf A)$ If $\\mathcal C(\\mathbf A)\\subset \\mathcal C(\\mathbf B)$, then there exista a matrix $\\mathbf C$ such that $\\mathbf A=\\mathbf B\\mathbf C$ Def: The null space of a matrix, denoted by $\\mathcal N(\\mathbf A)$, is defined by $\\mathcal N(\\mathbf A)=\\{\\mathbf y:\\mathbf A\\mathbf y=\\mathbf 0}$. Notice that if the matrix $\\mathbf A$ is $m\\times n$, then vectors in $\\mathcal N(\\mathbf A)$ have dimension $n$ while vectors in $\\mathcal C(\\mathbf A)$ have dimension $m$. Mathematically, this may be expressed as $\\mathcal C(\\mathbf A)\\subset \\mathbb R^m$ and $\\mathcal N(\\mathbf A)\\subset \\mathbb R^n$. If $\\mathbf A$ has full-column rank, then $\\mathcal N(\\mathbf A)=\\{\\mathbf 0\\}$ Suppose $\\mathbf A\\in\\mathbb R ^{m\\times n}$, then $\\dim(\\mathcal N(\\mathbf A))=n-r$ where $r=rank(\\mathbf A)$, or, more elegantly, $$ \\dim(\\mathcal N(\\mathbf A))+\\dim(\\mathcal C(\\mathbf A))=n. $$ Def: Two vector spaces $\\mathcal S$ and $\\mathcal T$ form orthogonal complements in $\\mathbb R^m$ if and only if $\\mathcal S$, $\\mathcal T\\subset\\mathbb R^m$, $\\mathcal S\\cap \\mathcal T={\\mathbf 0}$, $\\dim(\\mathcal S)=r$, $\\dim(\\mathcal T)=n-r$, and every vector in $\\mathcal S$ is orthogonal to every vector in $\\mathcal T$. Let $\\mathcal S$ and $\\mathcal T$ be orthogonal complements in $\\mathbb R^m$, then any vector $\\mathbf x\\in\\mathbb R^m$ can be written as $\\mathbf x=\\mathbf s+\\mathbf t$ where $\\mathbf s\\in\\mathcal S$ and $\\mathbf t\\in\\mathcal T$, and this decomposition is unique If $\\mathbf A$ is an $m\\times n$ matrix, then $\\","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:1:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Chapter 1: The GLM and Examples GLM: $\\mathbf y=\\mathbf X\\mathbf b+\\mathbf e$ $\\mathbf y$: $N\\times 1$ random vector of observable responses $\\mathbf X$: $N\\times p$ matrix of known constants $\\mathbf b$: $p\\times1$ vector of unknown constants $\\mathbf e$: $N\\times 1$ random vector of unobservable error such that $\\mathbb E(\\mathbf e)=\\mathbf 0$ It is called a linear model because the expected value of $\\mathbf y$, $\\mathbb E(\\mathbf y)$, is linear in $\\mathbf b$. Define $\\mu(\\mathbf b)=\\mathbf X\\mathbf b=\\mathbb E(\\mathbf y)$. $\\mu(\\cdot)$ is a linear function: Fix $\\mathbf b^{(1)}$, $\\mathbf b^{(2)}\\in\\mathbb R^p$, and $a_1,a_2\\in\\mathbb R$, then $$ \\mu(a_1\\mathbf b^{(1)}+a_2\\mathbf b^{(2)})=\\mathbf X(a_1\\mathbf b^{(1)}+a_2\\mathbf b^{(2)})=a_1\\mathbf X\\mathbf b^{(1)}+a_2\\mathbf X\\mathbf b^{(2)}=a_1\\mu(\\mathbf b^{(1)})+a_2\\mu(\\mathbf b^{(2)}) $$ Examples: One-sample problem (p=1): $ y_1,\\cdots,y_N$ i.i.d., $\\mathbb E(y_1)=\\mu$, $Var( y_1)=\\sigma^2$. This is a special case of the GLM with $\\mathbf y=(y_1,\\cdots,y_N)^T$ and $\\mathbf X\\mathbf b=\\mathbf 1_N\\mu$; $\\mathbf e$ is just a vector of i.i.d. centered errors $e_1\\stackrel{d}{=}y_1-\\mu$. Simple linear regression (p=2): $y_i=\\beta_0+\\beta_1x_i+e_i$, $i=1,2,\\cdots,N$, where $\\{e_i\\}_{i=1}^N$ uncorrelated r.v.s with $\\mathbb E(e_i)=0$ and $Var(e_i)=\\sigma^2$. This is a special case of the GLM with $\\mathbf y=(y_1,\\cdots,y_N)^T$, $\\mathbf X\\mathbf b=\\begin{pmatrix}1\u0026x_0\\\\\\vdots\u0026\\vdots\\\\1\u0026x_N\\end{pmatrix}\\begin{pmatrix}\\beta_0\\\\\\beta_1\\end{pmatrix}$, $\\mathbf e=(e_1,\\cdots,e_N)^T$. Actually we are making the assumption that the $x_i$’s are measured without error. Eg. $y_i=\\beta_0+\\beta_1n_i+e_i$ (True Model) $y_i$ is the crop yield, and $n_i$ is the nitrogen in soil. We don’t see $n_i$ exactly; we see a distorted version $x_i=n_i+u_i$, where $u_i$ is a random measurement error. $y_i=\\beta_0+\\beta_1(n_i+u_i)+e_i$ (Observed Model) $\\rightarrow$ Is this a special case of the GLM? No! Multiple regression (p=k+1): $y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_kx_{ik}+e_i$, $i=1,2,\\cdots,N$, where $\\{e_i\\}^N_{i=1}$ uncorrelated r.v.s with $\\mathbb E(e_i)=0$ and $Var(e_i)=\\sigma^2$. This is a special case of the GLM with $\\mathbf y=(y_1,\\cdots,y_N)^T$, $\\mathbf X\\mathbf b=\\begin{pmatrix}1\u0026x_{11}\u0026\\cdots\u0026x_{1k}\\\\\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\1\u0026x_{11}\u0026\\cdots\u0026x_{1k}\\end{pmatrix}\\begin{pmatrix}\\beta_0\\\\\\vdots\\\\\\beta_N\\end{pmatrix}$, $\\mathbf e=(e_1,\\cdots,e_N)^T$. $y_i=\\beta_0+\\beta_1i+\\beta_2i^2+e_i$ (Yes) $y_i=\\beta_0+\\beta_1\\cos\\left(\\frac{2\\pi i}{7}\\right)+\\beta_2\\log(i+1)+e_i$ (Yes) $y_i=\\beta_0+\\beta_1 e^{-\\beta_2x_i}+e_i$ (No) $y_i=\\frac{\\beta_0}{\\beta_1+\\beta_2 x_i}+e_i$ (No) One-way ANOVA: $y_{ij}=\\mu+\\alpha_i+e_{ij}$, $i=1,\\cdots, a$, $j=1,\\cdots, n_i$, where $\\{e_{ij}\\}^{a,n_i}_{i,j=1}$ uncorrelated r.v.s with $\\mathbb E(e_{ij})=0$ and $Var(e_{ij})=\\sigma^2$. If we assume that $\\{\\alpha_i\\}^a_{i=1}$ are unknown constants, then this is a special case of the GLM with $\\mathbf y=(y_{11},\\cdots,y_{1n_1},y_{21},\\cdots,y_{2n_2},\\cdots,y_{a1},\\cdots, y_{an_a})^T$, $\\mathbf X\\mathbf b=\\begin{pmatrix}\\mathbf 1_{n_1}\u0026\\mathbf 1_{n_1}\u0026\\mathbf0\u0026\\cdots\u0026\\mathbf 0\\\\\\mathbf 1_{n_2}\u0026\\mathbf 0\u0026\\mathbf 1_{n_2}\u0026\\cdots\u0026\\mathbf 0\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\\mathbf 1_{n_a}\u0026\\mathbf0\u0026\\mathbf0\u0026\\cdots\u0026\\mathbf1_{n_a}\\end{pmatrix}\\begin{pmatrix}\\mu\\\\\\alpha_1\\\\\\vdots\\\\\\alpha_a\\end{pmatrix}$, $\\mathbf e=(e_{11},\\cdots,e_{1n_1},e_{21},\\cdots,e_{2n_2},\\cdots,e_{a1},\\cdots, e_{an_a})^T$. The comments: $\\mathbf X$ is not full rank If the $a$ treatments were randomly selected from a population of treatments, then $\\{\\alpha_i\\}_{i=1}^a$ must be considered random, and in this case, we have a so-called mixed model. However, if we redefine $\\mathbf X\\mathbf b$ and $\\mathbf e$, this mixed model is still a special case of the GLM with $\\mathbf X\\mathbf b=\\begin{pmatrix}1\\\\1\\\\\\vdots\\\\1\\end{pmatrix}\\mu$, $\\mathbf e=\\begin{pmatrix}\\alpha_1+e_{11}\\\\\\alpha_1+e_{12}\\\\\\vdots\\\\\\alpha_a+e_{a1}\\\\\\vdots\\\\\\alpha_a+e","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:2:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Chapter 2: The Linear Least Squares Problem ","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:3:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"2.1 The Normal Equations Assumption: $\\mathbf y\\in\\mathbb R^N$ and $\\mathbf X\\in\\mathbb R^{N\\times p}$ Goal: Find the value of $\\mathbf b\\in\\mathbb R^p$ that minimizes $$ Q(\\mathbf b)=(\\mathbf y-\\mathbf X\\mathbf b)^T(\\mathbf y-\\mathbf X\\mathbf b)=|\\mathbf y-\\mathbf X\\mathbf b|^2 $$ A value of $\\mathbf b$ that minimizes $Q(\\mathbf b)$ will be called a least squares solution $$ \\mathcal C(\\mathbf X)=\\{\\mathbf v\\in\\mathbb R^N:\\mathbf v=\\mathbf X\\mathbf w\\text{ for some }\\mathbf w\\in\\mathbb R^p\\} $$ Mathematically, trying to find the closest point in $\\mathcal C(\\mathbf X)$ to the data $\\mathbf y$ The gradient vector is given by $$ \\frac{\\partial Q}{\\partial \\mathbf b}=(\\frac{\\partial Q}{\\partial b_1},\\frac{\\partial Q}{\\partial b_2},\\cdots,\\frac{\\partial Q}{\\partial b_p})^T $$ The minimum of $Q$ will occur where the gradient is zero, so we want to solve $$ \\frac{\\partial Q}{\\partial\\mathbf b}=\\mathbf 0 $$ Result 2.1: Assume $\\mathbf a$ is a $p\\times 1$ vector, $\\mathbf b$ is a $p\\times1$ vector, $\\mathbf A$ is a $p\\times p$ matrix. Then $\\frac{\\partial \\mathbf a^T\\mathbf b}{\\partial \\mathbf b}=\\mathbf a$ $\\frac{\\partial\\mathbf b^T\\mathbf A\\mathbf b}{\\partial \\mathbf b}=(\\mathbf A+\\mathbf A^T)\\mathbf b$ proof: For the first result, note that $\\mathbf a^T\\mathbf b=a_1b_1+a_2b_2+\\cdots+a_pb_p$, so $\\left(\\frac{\\partial\\mathbf a^T\\mathbf b}{\\partial \\mathbf b}\\right)_j=\\frac{\\partial\\mathbf a^T\\mathbf b}{\\partial\\mathbf b_j}=a_j$. Similarly, $$ \\mathbf b^T\\mathbf A\\mathbf b=\\sum_{j=1}^p\\sum_{k=1}^pA_{jk}b_jb_k, $$ and to form $\\left(\\frac{\\partial\\mathbf b^T\\mathbf A\\mathbf b}{\\partial\\mathbf b}\\right)_j$, the terms that depend on $b_j$ are $$ A_{jj}b_j^2+\\sum_{k\\not=j}(A_{jk}+A_{kj})b_kb_j, $$ so the partial derivative w.r.t. $b_j$ is $$ 2A_{jj}b_j+\\sum_{k\\not=j}A_{jk}b_k+\\sum_{k\\not=j}A_{kj}b_k, $$ which is the $j$-th element of $(\\mathbf A+\\mathbf A^T)\\mathbf b$. Back to $Q(\\mathbf b)$, using Result 2.1, $$ \\frac{\\partial Q}{\\partial \\mathbf b}=-2\\mathbf X^T\\mathbf y+2\\mathbf X^T\\mathbf X\\mathbf b. $$ Now, setting the gradient to 0 yields $$ \\mathbf X^T\\mathbf X\\mathbf b=\\mathbf X^T\\mathbf y. $$ They are the normal equations. Example: SLR: $y_i=\\beta_0+\\beta_1x_i+e_i$, $i=1,\\cdots,N$ $$ \\mathbf X^T\\mathbf X\\mathbf b=\\begin{pmatrix}N\u0026N\\bar{\\mathbf x}\\\\N\\bar{\\mathbf x}\u0026\\sum_{i=1}^Nx_i^2\\end{pmatrix}\\begin{pmatrix}\\beta_0\\\\\\beta_1\\end{pmatrix},\\quad \\mathbf X^T\\mathbf y=\\begin{pmatrix}N\\bar{\\mathbf y}\\\\\\sum_{i=1}^Nx_iy_i\\end{pmatrix} $$ Assume that $\\sum_{i=1}^N(x_i-\\bar{\\mathbf x})^2\u003e0$, i.e., the $x_i$’s are NOT all the same. The solution to the N.E. is $$ \\hat \\beta_1=\\frac{\\sum_{i=1}^N(x_i-\\bar{\\mathbf x})y_i}{\\sum_{i=1}^N(x_i-\\bar{\\mathbf x})^2}\\\\ \\hat\\beta_0=\\bar{\\mathbf y}-\\hat \\beta_1\\bar{\\mathbf x} $$ If $\\sum_{i=1}^N(x_i-\\bar{\\mathbf x})^2=0$, then there are infinitely many solutions to the N.E.s $$ \\hat\\beta_1=c\\\\\\hat\\beta_0=\\bar{\\mathbf y}-c\\bar{\\mathbf x} $$ Balanced one-way ANOVA: $y_{ij}=\\mu+\\alpha_i+e_{ij}$, $i=1,\\cdots,a$, $j=1,\\cdots,n$ $$ \\mathbf X=\\begin{pmatrix}\\mathbf 1_n\u0026\\mathbf 1_n\u0026\\mathbf 0\u0026\\cdots\u0026\\mathbf 0\\\\\\mathbf 1_n\u0026\\mathbf 0\u0026\\mathbf 1_n\u0026\\cdots\u0026\\mathbf 0\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\\\mathbf 1_n\u0026\\mathbf 0\u0026\\mathbf 0\u0026\\cdots\u0026\\mathbf 1_n\\end{pmatrix} $$ $$ \\mathbf X^T\\mathbf X\\mathbf b=\\begin{pmatrix}n\u0026n\u0026n\u0026\\cdots\u0026n\\\\n\u0026n\u00260\u0026\\cdots\u00260\\\\n\u00260\u0026n\u0026\\cdots\u00260\\\\\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\\\\n\u0026 0\u0026 0\u0026\\cdots\u0026n\\end{pmatrix}\\begin{pmatrix}\\mu\\\\\\alpha_1\\\\\\vdots\\\\\\alpha_a\\end{pmatrix},\\quad \\mathbf X^T\\mathbf y=\\begin{pmatrix}y_{\\cdot\\cdot}\\\\y_{1\\cdot}\\\\\\vdots\\\\y_{a\\cdot}\\end{pmatrix} $$ $\\mathbf X^T\\mathbf X$ is singular, again, there are infinitely many solutions to the N.E.s $$ \\hat\\mu=c\\\\\\hat\\alpha_i=\\bar{y}_{i}-c $$ for any $c\\in\\mathbb R$. Next, we will show that the N.E.s are consistent and we’ll also show that $$ Q(\\mathbf b)\\text{ is minimized at }\\hat{\\mathbf b}\\quad\\Leftrightarrow\\quad \\hat{\\mathbf b}\\text{ is a solution to the N.E.s} $$ ","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:3:1","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"2.2 The Geometry of Least Squares Recall that a vector space, $\\mathcal S$, in $\\mathbb R^m$ is a collection of vectors that is closed under addition and scalar multiplication. So if $\\mathbf x^{(1)}, \\mathbf x^{(2)}\\in\\mathcal S$ and $c_1,c_2\\in\\mathbb R$, then $c_1\\mathbf x^{(1)}+c_2\\mathbf x^{(2)}\\in\\mathcal S$. Def A.4: Two vector spaces $\\mathcal S$ and $\\mathcal T$ form orthogonal complements in $\\mathbb R^m$ if and only if $\\mathcal S$, $\\mathcal T\\subset\\mathbb R^m$, $\\mathcal S\\cap \\mathcal T={\\mathbf 0}$, $\\dim(\\mathcal S)+\\dim(\\mathcal T)=n$, and $\\mathbf s^T\\mathbf t=0$ for every vector $\\mathbf s$ in $\\mathcal S$ and every vector $\\mathbf t$ in $\\mathcal T$. Result A.4: Let $\\mathcal S$ and $\\mathcal T$ be orthogonal complements in $\\mathbb R^m$, then any vector $\\mathbf x\\in\\mathbb R^m$ can be written as $\\mathbf x=\\mathbf s+\\mathbf t$ where $\\mathbf s\\in\\mathcal S$ and $\\mathbf t\\in\\mathcal T$, and this decomposition is unique Result A.5: If $\\mathbf A$ is an $m\\times n$ matrix, then $\\mathcal C(\\mathbf A)$ and $\\mathcal N(\\mathbf A^T)$ are orthogonal complements in $\\mathbb R^m$ Result A.6: Let $\\mathcal S_1$ and $\\mathcal T_1$ be orthogonal complements, as welle as $\\mathcal S_2$ and $\\mathcal T_2$; then if $\\mathcal S_1\\subset \\mathcal S_2$, then $\\mathcal T_2\\subset \\mathcal T_1$ Question: Are the N.E.s consistent? In other words, is it always true that $\\mathbf X^T\\mathbf y\\in\\mathcal C(\\mathbf X^T\\mathbf X)$? Lemma 2.1: $\\mathcal N(\\mathbf X^T\\mathbf X)=\\mathcal N(\\mathbf X)$ proof: $\\mathbf w\\in\\mathcal N(\\mathbf X)\\Rightarrow \\mathbf X\\mathbf w=\\mathbf 0\\Rightarrow \\mathbf X^T\\mathbf X\\mathbf w=\\mathbf 0\\Rightarrow \\mathbf w\\in\\mathcal N(\\mathbf X^T\\mathbf X)$ $\\mathbf w\\in\\mathcal N(\\mathbf X^T\\mathbf X)\\Rightarrow \\mathbf X^T\\mathbf X\\mathbf w=\\mathbf 0\\Rightarrow \\mathbf w^T\\mathbf X^T\\mathbf X\\mathbf w=0\\Rightarrow |\\mathbf X\\mathbf w|=0\\Rightarrow \\mathbf X\\mathbf w=\\mathbf 0\\Rightarrow \\mathbf w\\in\\mathcal N(\\mathbf X)$ Result 2.2: $\\mathcal C(\\mathbf X^T\\mathbf X)=\\mathcal C(\\mathbf X^T)$ proof: Result A.5 $\\Rightarrow$$\\mathcal N(\\mathbf X)$ and $\\mathcal C(\\mathbf X^T)$ are O.C.s, $\\mathcal N(\\mathbf X^T\\mathbf X)$ and $\\mathcal C(\\mathbf X^T\\mathbf X)$ are O.C.s Result A.6 $\\Rightarrow$ $\\mathcal N(\\mathbf X)\\subset \\mathcal N(\\mathbf X^T\\mathbf X)\\Rightarrow \\mathcal C(\\mathbf X^T\\mathbf X)\\subset\\mathcal C(\\mathbf X^T)$, $\\mathcal N(\\mathbf X^T\\mathbf X)\\subset \\mathcal N(\\mathbf X)\\Rightarrow \\mathcal C(\\mathbf X^T)\\subset\\mathcal C(\\mathbf X^T\\mathbf X)$ Corollary 2.1: The N.E.s are consistent proof: Result 2.2: $\\mathbf X^T\\mathbf y\\in\\mathcal C(\\mathbf X^T)=\\mathcal C(\\mathbf X^T\\mathbf X) $ Result 2.3: $\\hat{\\mathbf b}$ is a solution to the N.E.s iff $\\hat{\\mathbf b}$ minimizes $Q(\\mathbf b)=|\\mathbf y-\\mathbf X\\mathbf b|^2$ *By Result A.12, $\\hat{\\mathbf b}=(\\mathbf X^T\\mathbf X)^g\\mathbf X^T\\mathbf y$ is a solution to the N.E.s proof: By Corollary 2.1, we know that there exists a solution to the N.E.s, $\\hat{\\mathbf b}$. $$ \\begin{aligned} Q(\\mathbf b)\u0026=(\\mathbf y-\\mathbf X\\mathbf b)^T(\\mathbf y-\\mathbf X\\mathbf b)\\\\ \u0026=(\\mathbf y-\\mathbf X\\hat{\\mathbf b}+\\mathbf X\\hat{\\mathbf b}-\\mathbf X\\mathbf b)^T(\\mathbf y-\\mathbf X\\hat{\\mathbf b}+\\mathbf X\\hat{\\mathbf b}-\\mathbf X\\mathbf b)\\\\ \u0026=(\\mathbf y-\\mathbf X\\hat{\\mathbf b})^T(\\mathbf y-\\mathbf X\\hat{\\mathbf b})+2(\\hat{\\mathbf b}-\\mathbf b)^T\\mathbf X^T(\\mathbf y-\\mathbf X\\hat{\\mathbf b})+(\\mathbf X\\hat{\\mathbf b}-\\mathbf X\\mathbf b)^T(\\mathbf X\\hat{\\mathbf b}-\\mathbf X\\mathbf b)\\\\ \u0026=Q(\\hat{\\mathbf b})+|\\mathbf X(\\hat{\\mathbf b}-\\mathbf b)|^2 \\end{aligned} $$ where the cross-product term vanishes since $\\hat{\\mathbf b}$ solves the N.E.s $\\mathbf X^T(\\mathbf y-\\mathbf X^T\\mathbf b)=\\mathbf 0$. Now since no other value of $\\mathbf b$ can give a smaller value of $Q(\\mathbf b)$, clearly $\\hat{\\mathbf b}$ minimizes $Q(\\mathbf b)$. For the other direction, we still have $\\hat{\\mathbf b}$ that solves N.E.s, and we now know that $\\hat{\\mathbf b}$ mi","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:3:2","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"2.3 Reparametrization One-way ANOVA: $y_{ij}=\\mu+\\alpha_i+e_{ij}$, $i=1,2,3$ $y_{ij}=d_i+e_{ij}$ $y_{ij}=c_1+c_2\\mathbb 1_{\\{1\\}}(i)+c_3\\mathbb 1_{\\{2\\}}(i)+e_{ij}$ These three versions of the ANOVA model should lead ti equivalent inferences. Def: Two L.M.s, $\\mathbf y=\\mathbf X\\mathbf b+\\mathbf e$, where $\\mathbf X$ is $N\\times p$, and $\\mathbf y=\\mathbf W\\mathbf c+\\mathbf e$, where $\\mathbf W$ is $N\\times t$, are equivalent or reparametrizations of each other, if the two design matrices, $\\mathbf X$ and $\\mathbf W$, have the same column sapce, i.e., $\\mathcal C(\\mathbf X)=\\mathcal C(\\mathbf W)$. Back to ANOVA examples: $$ \\mathbf X\\mathbf b=\\begin{pmatrix}\\mathbf 1_{n_1}\u0026\\mathbf 1_{n_1}\u0026\\mathbf 0\u0026\\mathbf 0\\\\ \\mathbf 1_{n_2}\u0026\\mathbf 0\u0026\\mathbf 1_{n_2}\u0026\\mathbf 0\\\\\\mathbf 1_{n_3}\u0026\\mathbf 0\u0026\\mathbf 0\u0026\\mathbf 1_{n_3}\\end{pmatrix}\\begin{pmatrix}\\mu\\\\\\alpha_1\\\\\\alpha_2\\\\\\alpha_3\\end{pmatrix} $$ $$ \\mathbf W\\mathbf c=\\begin{pmatrix}\\mathbf 1_{n_1}\u0026\\mathbf 1_{n_1}\u0026\\mathbf 0\\\\\\mathbf 1_{n_2}\u0026\\mathbf 0\u0026\\mathbf 1_{n_2}\\\\\\mathbf 1_{n_3}\u0026\\mathbf 0\u0026\\mathbf 0\\end{pmatrix}\\begin{pmatrix}c_1\\\\c_2\\\\c_3\\\\\\end{pmatrix} $$ $\\mathcal C(\\mathbf X)=\\mathcal C(\\mathbf W)$: Note that in $\\mathbf X$, column 4=column1-column2-column3. Result 2.8: If two L.M.s are equivalent, then $\\mathbf P_\\mathbf X=\\mathbf P_\\mathbf W$. Corollary 2.4: If two L.M.s are equivalent, then $\\hat{\\mathbf y}$ and $\\hat{\\mathbf e}$ are the same for the two models. proof: $\\hat{\\mathbf y}=\\mathbf P_\\mathbf X\\mathbf y$, $\\hat{\\mathbf e}=(\\mathbf I-\\mathbf P_\\mathbf X)\\mathbf y$. Assume $\\mathcal C(\\mathbf X)=\\mathcal C(\\mathbf W)$. Result A.2 implies the existence of two matrices, $\\mathbf S$ and $\\mathbf T$, such that $\\mathbf W=\\mathbf X\\mathbf T$ and $\\mathbf X=\\mathbf W\\mathbf S$. Result 2.9: Assume $\\mathcal C(\\mathbf X)=\\mathcal C(\\mathbf W)$. If $\\hat{\\mathbf c}$ solves the N.E.s $\\mathbf W^T\\mathbf W\\mathbf c=\\mathbf W^T\\mathbf y$, then $\\hat{\\mathbf b}=\\mathbf T\\hat{\\mathbf c}$ solves the N.E.s $\\mathbf X^T\\mathbf X\\mathbf b=\\mathbf X^T\\mathbf y$. proof: $$ \\mathbf X^T\\mathbf X\\mathbf T\\hat{\\mathbf c}=\\mathbf X^T\\mathbf W\\hat{\\mathbf c}=\\mathbf X^T\\mathbf P_\\mathbf W\\mathbf y=\\mathbf X^T\\mathbf P_\\mathbf X\\mathbf y=\\mathbf X^T\\mathbf y. $$ Example: $y_{ij}=\\mu+\\alpha_i+e_{ij}$, $i=1,2,3$. $$ \\hat{\\mathbf b}=\\begin{pmatrix}\\bar{y}_{\\cdot\\cdot}\\\\ \\bar{y}_{1\\cdot}-\\bar{y}_{\\cdot\\cdot}\\\\\\bar{y}_{2\\cdot}-\\bar{y}_{\\cdot\\cdot}\\\\\\bar{y}_{3\\cdot}-\\bar{y}_{\\cdot\\cdot}\\end{pmatrix} $$ General solution: $$ \\begin{pmatrix}0\\\\\\bar{y}_{1\\cdot}\\\\\\bar{y}_{2\\cdot}\\\\\\bar{y}_{3\\cdot}\\end{pmatrix}+z\\begin{pmatrix}1\\\\-1\\\\-1\\\\-1\\end{pmatrix},\\quad\\text{for any }z\\in\\mathbb R $$ $y_{ij}=c_1+c_2\\mathbf I_{\\{1\\}}(i)+c_3\\mathbf I_{\\{2\\}}(i)+e_{ij}$, $i=1,2,3$ $$ \\hat{c}=\\begin{pmatrix}\\bar{y}_{3\\cdot}\\\\\\bar{y}_{1\\cdot}-\\bar{y}_{2\\cdot}\\\\\\bar{y}_{2\\cdot}-\\bar{y}_{3\\cdot}\\end{pmatrix} $$ For $\\mathbf W$ and $\\mathbf X$ presented before, we have $\\mathbf W=\\mathbf X\\mathbf T$, where $\\mathbf T$ is give by $$ \\mathbf T=\\begin{pmatrix}1\u00260\u00260\\\\0\u00261\u00260\\\\0\u00260\u00261\\\\0\u00260\u00260\\end{pmatrix} $$ $$ \\hat{\\mathbf b}=\\mathbf T\\hat{\\mathbf c}=\\begin{pmatrix}1\u00260\u00260\\\\0\u00261\u00260\\\\0\u00260\u00261\\\\0\u00260\u00260\\end{pmatrix}\\begin{pmatrix}\\bar{y}_{3\\cdot}\\\\\\bar{y}_{1\\cdot}-\\bar{y}_{2\\cdot}\\\\\\bar{y}_{2\\cdot}-\\bar{y}_{3\\cdot}\\end{pmatrix}=\\begin{pmatrix}\\bar{y}_{3\\cdot}\\\\\\bar{y}_{1\\cdot}-\\bar{y}_{2\\cdot}\\\\\\bar{y}_{2\\cdot}-\\bar{y}_{3\\cdot}\\\\0\\end{pmatrix} $$ 2.4 A Version of the QR Decomposition Result: Let $\\mathbf W_{N\\times p}$ have full column rank. Then $\\mathbf W$ can be decomposed as $\\mathbf W=\\mathbf Q\\mathbf R$ where $\\mathbf Q_{N\\times p}$ is s.t. $\\mathbf Q^T\\mathbf Q=\\mathbf I_p$ and $\\mathbf R_{p\\times p}$ is upper triangular, with positive elements on the diagonal. Idea of the proof: Write the columns of $\\mathbf W$ as $\\mathbf W_{\\cdot1},\\mathbf W_{\\cdot2},\\cdots,\\mathbf W_{\\cdot p}$. We begin by constructing a set of orthogonal vectors $\\mathbf U_{\\cdot1},\\mathbf U_{\\cdot2},\\cdots,\\mathbf U_{\\cdot p}$, each of which is a linear combinatio","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:3:3","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Chapter 3: Estimability and Least Squares Estimators GLM: $\\mathbf y=\\mathbf X\\mathbf b+\\mathbf e$ We assume $\\mathbb E\\mathbf e=0$, so $\\mathbb E\\mathbf y=\\mathbf X\\mathbf b$. Fix $\\pmb\\lambda\\in\\mathbb R^p$ Def 3.1: An estimator $t(\\mathbf y)$ is unbiased for $\\pmb\\lambda^T\\mathbf b$ if $\\mathbf Et(\\mathbf y)=\\pmb\\lambda^T\\mathbf b$, $\\forall \\mathbf b\\in\\mathbb R^p$. Def 3.2: An estimator $t(\\mathbf y)$ is a linear estimator if $t(\\mathbf y)=c+\\mathbf a^T\\mathbf y$ where $c\\in\\mathbb R$ and $\\mathbf a\\in\\mathbb R^p$ are known constants. Def 3.3: A function $\\pmb\\lambda^T\\mathbf b$ is (linearly) estimable if $\\exist$ a linear unbiased estimator of it; otherwise $\\pmb\\lambda^T\\mathbf b$ is called non-estimable. Result 3.1: $\\pmb\\lambda^T\\mathbf b$ is estimable iff $\\exist \\mathbf a\\in\\mathbb R^N$ such that $\\pmb\\lambda=\\mathbf X^T\\mathbf a$, or, in other words, $\\pmb\\lambda\\in\\mathcal C(\\mathbf X^T)$. proof: ($\\Leftarrow$) Assume there exists $\\mathbf a\\in\\mathbb R^N$ such that $\\mathbf X^T\\mathbf a=\\pmb\\lambda$. Then $\\mathbb E(\\mathbf a^T\\mathbf y)=\\mathbf a^T\\mathbb E(\\mathbf y)=\\mathbf a^T\\mathbf X\\mathbf b=\\pmb\\lambda^T\\mathbf b$. ($\\Rightarrow$) Assume that $\\pmb\\lambda^T\\mathbf b$ is estimable. Then $\\exist c,\\mathbf a$ such that $\\mathbb E(c+\\mathbf a^T\\mathbf y)=\\pmb\\lambda^T\\mathbf b$, $\\forall \\mathbf b\\in\\mathbb R^p$. Then, $c+\\mathbf a^T\\mathbf X\\mathbf b-\\pmb\\lambda^T\\mathbf b=0$, $\\forall \\mathbf b\\in\\mathbb R^p\\Rightarrow c+(\\mathbf a^T\\mathbf X-\\pmb\\lambda^T)\\mathbf b=0$, $\\forall \\mathbf b\\in\\mathbb R^p$. Using Result A.8, we have $c=0$ and $\\mathbf a^T\\mathbf X-\\pmb\\lambda^T=\\mathbf 0$ or $\\pmb\\lambda=\\mathbf X^T\\mathbf a$. Example 3.1: One-way ANOVA with $a=n=2$. $y_{ij}=\\mu+\\alpha_i+e_{ij}$, $i=1,2$, $j=1,2$. $\\mathbf X\\mathbf b=\\begin{pmatrix}1\u00261\u00260\\\\1\u00261\u00260\\\\1\u00260\u00261\\\\1\u00260\u00261\\end{pmatrix}\\begin{pmatrix}\\mu\\\\\\alpha_1\\\\\\alpha_2\\end{pmatrix}$. Q: Is $\\alpha_1$ estimable? A: First, $\\alpha_1=\\pmb\\lambda^T\\mathbf b=(0,1,0)(\\mu,\\alpha_1,\\alpha_2)^T$. Is $(0,1,0)^T\\in\\mathcal C(\\mathbf X^T)$​? $$ \\begin{pmatrix} 1\u00261\u00261\u00261\\\\1\u00261\u00260\u00260\\\\0\u00260\u00261\u00261 \\end{pmatrix} \\begin{pmatrix} a_1\\\\a_2\\\\a_3\\\\a_4 \\end{pmatrix}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix} $$ $$ \\Rightarrow \\sum_{i=1}^4a_i=0,\\quad a_1+a_2=1,\\quad a_3+a_4=0 $$ This is impossible. So $(0,1,0)^T\\not\\in\\mathcal C(\\mathbf X^T)\\Rightarrow\\alpha_1$ is not estimable. Result 3.1a: $\\pmb\\lambda^T\\mathbf b$ is estimable if and only if $\\pmb\\lambda^T\\mathbf b$ is a linear combination of the expected values of the $ y_i$’s. proof: ($\\Rightarrow$) Assume that $\\pmb\\lambda^T\\mathbf b$ is estimable. Then $\\exist$ $a\\mathbf \\in\\mathbb R^N$ such that $\\pmb\\lambda=\\mathbf X^T\\mathbf a$. Then $\\pmb\\lambda^T\\mathbf b=\\mathbf a^T\\mathbf X\\mathbf b=\\mathbf a^T\\mathbb E(\\mathbf y)$. ($\\Leftarrow$) Assume that $\\pmb\\lambda^T\\mathbf b$ is a linear combination of the elements of $\\mathbf X\\mathbf b$. Then, for all $\\mathbf b\\in\\mathbb R^p$, $\\pmb\\lambda^T\\mathbf b=\\mathbf a^T\\mathbf X\\mathbf b\\Rightarrow\\pmb\\lambda^T=\\mathbf a^T\\mathbf X$ or $\\pmb\\lambda=\\mathbf X^T\\mathbf a$, i.e., $\\pmb\\lambda\\in\\mathcal C(\\mathbf X^T)$. So $\\pmb\\lambda^T\\mathbf b$ is estimable. *If we take $\\mathbf a^T=(\\mathbf e^{(i)})^T$, then $\\mathbf a^T\\mathbb E\\mathbf y=\\mathbb E y_i$. So the expected value of any $y_i$ is estimable. Suppose $\\pmb\\lambda^{(j)}\\in\\mathbb R^p$, $j=1,2,\\cdots,k$. If $\\pmb\\lambda^{(j)T}\\mathbf b$ are all estimable, so is $\\sum_{j=1}^kd_j\\pmb\\lambda^{(j)T}\\mathbf b=\\left[\\sum_{j=1}^kd_j\\pmb\\lambda^{(j)}\\right]^T\\mathbf b$ for any $\\{d_j\\}_{j=1}^k$. Def: A set of estimable functions $\\{\\pmb\\lambda^{(j)}\\mathbf b\\}_{j=1}^k$ is called linearly independent if the $\\pmb\\lambda^{(j)}$’s are linearly independent. Let $r=rank(\\mathbf X^T)\\leq p$ and $\\dim(\\mathcal C(\\mathbf X^T))=rank(X^T)=r$. Thus, any set of L.I. estimable functions cannot have more than $r$ members. Suppose that $\\mathbf X$ has full column rank, $r=p$. Then $\\mathcal C(\\mathbf X^T)=\\mathbb R^p$ and $\\pmb\\lambda^T\\mathbf b$ is estimab","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:4:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Chapter 4: Gauss–Markov Model Suppose $\\Sigma$ is an $n\\times n$ covariance matrix associated with $Z=(Z_1,\\cdots,Z_n)^T$. Then for $v\\in\\mathbb R^n$, we have $$ v^T\\Sigma v=Var(v^TZ)=Var(\\sum_{i=1}^nv_iZ_i)\\geq0\\quad\\Rightarrow\\quad\\Sigma\\text{ is non-negative definite}. $$ When is $\\Sigma$ positive definite? The only way that $v^T\\Sigma v=0$ when $v\\not=0$ is if $Var(\\sum_{i=1}^nv_iZ_i)=0$. This means that $\\sum_{i=1}^nv_iZ_i$ must be constant. If no such relationship exists, then $\\Sigma$ is positive definite. Gauss-Markov Model $\\mathbf y=\\mathbf X\\mathbf b+\\mathbf e$, $\\mathbb E\\mathbf e=0$, $Cov(\\mathbf e)=\\sigma^2\\mathbf I$. So, each element of $\\mathbf e$ has mean $0$, variance $\\sigma^2$ and $Cov(e_i,e_j)=0(i\\not=j)$. Let’s derive the variance of the L.S. estimator of the estimable function $\\pmb\\lambda^T\\mathbf b$. Of course, L.S. estimator of $\\pmb\\lambda^T\\mathbf b$ is $\\pmb\\lambda^T\\hat{\\mathbf b}=\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\mathbf X^T\\mathbf y$. We know that $\\mathbb E(\\pmb\\lambda^T\\hat{\\mathbf b})=\\pmb\\lambda^T\\mathbf b$. $$ \\begin{aligned} Var(\\pmb\\lambda^T\\hat{\\mathbf b})\u0026=Var(\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\mathbf X^T\\mathbf y)\\\\ \u0026=\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\mathbf X^TCov(\\mathbf y)[\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\mathbf X^T]^T\\\\ \u0026=\\sigma^2\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\mathbf X^T\\mathbf X[(\\mathbf X^T\\mathbf X)^g]^T\\pmb\\lambda \\end{aligned} $$ Note that $\\mathbf X^T\\mathbf X[(\\mathbf X^T\\mathbf X)^g]^T$ projects onto $\\mathcal C(\\mathbf X^T\\mathbf X)=\\mathcal C(\\mathbf X^T)$ and $\\pmb\\lambda\\in\\mathcal C(\\mathbf X^T)$, so $\\mathbf X^T\\mathbf X[(\\mathbf X^T\\mathbf X)^g]^T\\pmb\\lambda=\\pmb\\lambda$. So $$ Var(\\pmb\\lambda^T\\hat{\\mathbf b})=\\sigma^2\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\pmb\\lambda. $$ Example: SLR: $y_i=\\beta_0+\\beta_1X_i+e_i$. Find $Var(\\hat{\\beta}_1)$. $$ \\mathbf X^T\\mathbf X=\\begin{pmatrix}N\u0026N\\bar X\\\\N\\bar X\u0026\\sum_{i=1}^NX_i^2\\end{pmatrix} $$ $$ \\beta_1=\\pmb\\lambda^T\\mathbf b=\\begin{pmatrix}0\u00261\\end{pmatrix}\\begin{pmatrix}b_0\\\\b_1\\end{pmatrix} $$ $$ \\begin{aligned} Var(\\hat{\\beta}_1)\u0026=\\sigma^2\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\pmb\\lambda\\\\ \u0026=\\sigma^2\\begin{pmatrix}0\u00261\\end{pmatrix}\\begin{pmatrix}N\u0026N\\bar X\\\\N\\bar X\u0026\\sum_{i=1}^NX_i^2\\end{pmatrix}^{-1}\\begin{pmatrix}0\\\\1\\end{pmatrix}\\\\ \u0026=\\frac{\\sigma^2}{NS_{XX}}\\begin{pmatrix}0\u00261\\end{pmatrix}\\begin{pmatrix}\\sum_{i=1}^NX_i^2 \u0026 -N\\bar X\\\\-N\\bar X\u0026N\\end{pmatrix}\\begin{pmatrix}0\\\\1\\end{pmatrix}\\\\ \u0026=\\frac{\\sigma^2}{S_{XX}} \\end{aligned} $$ Theorem 4.1 (Gauss-Markov Theorem): Assume that $\\pmb\\lambda^T\\mathbf b$ is estimable. Under the G-M model, the L.S. estimator $\\pmb\\lambda^T\\hat{\\mathbf b}$ is the best (minimum variance) linear unbiased estimator (BLUE). proof: Suppose that $c+\\mathbf d^T\\mathbf y$ is an unbiased estimator of $\\pmb\\lambda^T\\mathbf b$. Then $c+\\mathbf d^T\\mathbf X\\mathbf b=\\pmb\\lambda^T\\mathbf b$, $\\forall \\mathbf b\\in\\mathbb R^p$. This implies $c=0$ and $\\mathbf d^T\\mathbf X=\\pmb\\lambda^T$. Now, $$ \\begin{aligned} \u0026Var(c+\\mathbf d^T\\mathbf y)\\\\ =\u0026Var(\\mathbf d^T\\mathbf y)\\\\ =\u0026Var(\\pmb\\lambda^T\\hat{\\mathbf b}+\\mathbf d^T\\mathbf y-\\pmb\\lambda^T\\hat{\\mathbf b})\\\\ =\u0026Var(\\pmb\\lambda^T\\hat{\\mathbf b})+Var(\\mathbf d^T\\mathbf y-\\pmb\\lambda^T\\hat{\\mathbf b})+2Cov(\\pmb\\lambda^T\\hat{\\mathbf b},\\mathbf d^T\\mathbf y-\\pmb\\lambda^T\\hat{\\mathbf b})\\\\ =\u0026Var(\\pmb\\lambda^T\\hat{\\mathbf b})+Var(\\mathbf d^T\\mathbf y-\\pmb\\lambda^T\\hat{\\mathbf b})+2Cov(\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\mathbf X^T\\mathbf y,[\\mathbf d-\\mathbf X[(\\mathbf X^T\\mathbf X)^g]^T\\pmb\\lambda]^T\\mathbf y)\\\\ =\u0026Var(\\pmb\\lambda^T\\hat{\\mathbf b})+Var(\\mathbf d^T\\mathbf y-\\pmb\\lambda^T\\hat{\\mathbf b})+2\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g\\mathbf X^TCov(\\mathbf y)[\\mathbf d-\\mathbf X[(\\mathbf X^T\\mathbf X)^g]^T\\pmb\\lambda]\\\\ =\u0026Var(\\pmb\\lambda^T\\hat{\\mathbf b})+Var(\\mathbf d^T\\mathbf y-\\pmb\\lambda^T\\hat{\\mathbf b})+2\\sigma^2\\pmb\\lambda^T(\\mathbf X^T\\mathbf X)^g[\\mathbf X^T\\mathbf d-\\mathbf X^T\\mathbf X[(\\mathbf X^T\\mathbf X)^g]^T\\pmb\\lambda]\\\\ =\u0026V","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:5:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Chapter 5: Distributional Theory 5.2 Multivariate Normal Distribution Let $\\mathbf X\\sim\\mathcal N(\\mu,\\sigma^2)$ and $\\mathbf Y=a\\mathbf X+b$ for $a,b\\in\\mathbb R$. Then $\\mathbf Y\\sim\\mathcal N(a\\mu+b,a^2\\sigma^2)$. What about the multivariate version of this? In math statistics class, we defined the MVN distribution through the density. We said $\\mathbf X\\sim\\mathcal N_p(\\pmb\\mu,\\mathbf V)$, where $\\pmb\\mu\\in\\R^p$ and $\\mathbf V_{p\\times p}$ is positive definite, if $$ p_\\mathbf X(\\mathbf x)=\\frac{1}{(2\\pi)^{p/2}|\\mathbf V|^{1/2}}e^{-\\frac{1}{2}(\\mathbf x-\\pmb\\mu)^T\\mathbf V^{-1}(\\mathbf x-\\pmb\\mu)}. $$ Now, what about a linear function of $\\mathbf X$? Suppose $\\mathbf A_{q\\times p}$ and $\\mathbf b\\in\\R^q$. Define $\\mathbf Y=\\mathbf A\\mathbf X+\\mathbf b$. Is $\\mathbf Y$ MVN? $$ Cov(\\mathbf Y)=\\mathbf ACov(\\mathbf X)\\mathbf A^T=\\mathbf A\\mathbf V\\mathbf A^T $$ If $q\u003ep$, then $\\mathbf A\\mathbf V\\mathbf A^T$ cannot be positive definite since $rank(\\mathbf A\\mathbf V\\mathbf A^T)\u003cq$. So $\\mathbf Y=\\mathbf A\\mathbf X+\\mathbf b$ need not be MVN in our old “density” sense. However, $\\mathbf Y=\\mathbf A\\mathbf X+\\mathbf b$ is MVN in a more general sense. Def: The mgf of a $p$-dimensional r.v. is defined as $$ M_\\mathbf X(t)=\\mathbb Ee^{\\mathbf t^T\\mathbf X} $$ provided there exists $h\u003e0$ such that this expectation exists (is finite) for all $\\mathbf t\\in S$, where $S:=\\{\\mathbf t\\in\\mathbb R^p:t_i\\in(-h,h),i=1,2,\\cdots,p\\}$. Def: Let $\\mathbf Z=(Z_1,\\cdots,Z_p)^T$ where $\\{Z_i\\}_{i=1}^p$ are i.i.d. $\\mathcal N(0,1)$. Then $\\mathbf Z$ has the standard multivariate normal distribution, denoted by $\\mathcal N_p(\\mathbf 0,\\mathbf I)$. Its pdf is given by $$ p_\\mathbf Z(\\mathbf z)=(2\\pi)^{-p/2}e^{-\\frac{1}{2}\\mathbf z^T\\mathbf z} $$ Q: What is the mgf of $\\mathbf Z$ (assuming existence)? Fix $\\mathbf t\\in\\R^p$, $$ \\begin{aligned} M_\\mathbf Z(\\mathbf t)=\\mathbb Ee^{\\mathbf t^T\\mathbf Z}\u0026=\\int_{\\R^p}(2\\pi)^{-p/2}e^{-\\frac{1}{2}\\mathbf z^T\\mathbf z+\\mathbf t^T\\mathbf z},d\\mathbf z\\\\ \u0026=\\int_{\\mathbb R^p}(2\\pi)^{-p/2}e^{-\\frac{1}{2}\\sum_{i=1}^n(z_i^2-2t_iz_i)},d\\mathbf z\\\\ \u0026=\\prod_{i=1}^p\\int_\\R(2\\pi)^{-1/2}e^{-\\frac{1}{2}(z_i^2-2t_iz_i)},dz_i\\\\ \u0026=\\prod_{i=1}^p\\int_\\mathbb R(2\\pi)^{-1/2}e^{-\\frac{1}{2}(z_i-t_i)^2}e^{\\frac{1}{2}t_i^2},dz_i\\\\ \u0026=\\prod_{i=1}^pe^{\\frac{1}{2}t_i^2}=e^{\\frac{1}{2}\\mathbf t^T\\mathbf t} \\end{aligned} $$ Note that the expectation exists (is finite) for all $\\mathbf t\\in\\mathbb R^p$. Result 5.1: Let $X_1$, $X_2$ be two r.v.s. If the mgf’s exist, and $M_{X_1}(t)=M_{X_2}(t)$ for all $t$ in an open square around the origin, then $X_1\\stackrel{d}{=}X_2$. Result 5.2: Suppose $X_1,X_2,\\cdots, X_p$ have mgfs $M_{X_i}(t_i),i=1,2,\\cdots,p$. Let $X=(X^T_1,X_2^T,\\cdots,X_p^T)^T$ have mgf $M_X(t)$, where $t=(t_1^T,t_2^T,\\cdots,t_p^T)^T$. Then $X_1,X_2,\\cdots,X_p$ are mutually independent iff $$ M_X(t)=\\prod_{i=1}^pM_{X_i}(t_i) $$ for all $t$ in an open square around the origin. Now suppose $\\mathbf Z\\sim\\mathcal N_p(\\mathbf 0,\\mathbf I)$ and let $\\mathbf X=\\pmb\\mu+\\mathbf A\\mathbf Z$. Then $$ M_\\mathbf X(\\mathbf t)=\\mathbb E\\left[e^{\\mathbf t^T(\\pmb\\mu+\\mathbf A\\mathbf Z)}\\right]=e^{\\mathbf t^T\\pmb\\mu}\\mathbb E[e^{\\mathbf t^T\\mathbf A\\mathbf Z}]\\\\=e^{\\mathbf t^T\\pmb\\mu}\\mathbb E[e^{(\\mathbf A^T\\mathbf t)^T\\mathbf Z}]=e^{\\mathbf t^T\\pmb\\mu}M_\\mathbf Z(\\mathbf A^T\\mathbf t)=e^{\\mathbf t^T\\pmb\\mu+\\frac{1}{2}\\mathbf t^T\\mathbf A\\mathbf A^T\\mathbf t}. $$ Thus, the distribution of $\\mathbf X=\\pmb\\mu+\\mathbf A\\mathbf Z$ depends on $\\mathbf A$ only through $\\mathbf A\\mathbf A^T$. Indeed, if $\\mathbf B\\mathbf B^T=\\mathbf A\\mathbf A^T$, then $\\pmb\\mu+\\mathbf A\\mathbf Z\\stackrel{d}{=}\\pmb\\mu+\\mathbf B\\mathbf Z$. Of course, $\\mathbf A\\mathbf A^T=Cov(\\mathbf X)$. Def: Let $\\pmb\\mu\\in\\R^p$ and let $\\mathbf V_{p\\times p}$ be non-negative definite. We say that $\\mathbf X\\sim\\mathcal N_p(\\pmb\\mu,\\mathbf V)$ if $$ M_\\mathbf X(\\mathbf t)=e^{\\mathbf t^T\\pmb\\mu+\\frac{1}{2}\\mathbf t^T\\mathbf V\\mathbf t}. $$ If $\\mathbf V$ is not positive definite, then ","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:6:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"Chapter 6: Statistical Inference 6.2 Results from Statistical Theory Assume $\\mathbf y\\sim\\mathcal N_N(\\mathbf X\\mathbf b,\\sigma^2\\mathbf I)$. $$ \\begin{aligned} f(\\mathbf y|\\mathbf b,\\sigma^2)\u0026=(2\\pi\\sigma^2)^{-N/2}\\exp\\{-\\frac{1}{2\\sigma^2}(\\mathbf y-\\mathbf X\\mathbf b)^T(\\mathbf y-\\mathbf X\\mathbf b)\\}\\\\ \u0026=(2\\pi\\sigma^2)^{-N/2}\\exp\\{-\\frac{1}{2\\sigma^2}\\mathbf b^T\\mathbf X^T\\mathbf X\\mathbf b\\}\\exp\\{-\\frac{1}{2\\sigma^2}\\mathbf y^T\\mathbf y+\\frac{1}{\\sigma^2}\\mathbf b^T\\mathbf X^T\\mathbf y\\} \\end{aligned} $$ By the factorization theorem, we see that $(\\mathbf y^T\\mathbf y,\\mathbf X^T\\mathbf y)$ is a sufficient statistic for $(\\mathbf b,\\sigma^2)$. Result 6.1: Assume $\\mathbf y\\sim\\mathcal N_N(\\mathbf X\\mathbf b,\\sigma^2\\mathbf I)$. $(\\mathbf y^T\\mathbf y,\\mathbf X^T\\mathbf y)$ is a minimal sufficient statistic for $(\\mathbf b,\\sigma^2)$. proof: According to Theorem 6.2.B in C\u0026B, it is enough to show that for two different responses, $\\mathbf y_1,\\mathbf y_2\\in\\mathbb R^N$, $\\mathbf y_1\\not=\\mathbf y_2$, $$ \\frac{f(\\mathbf y_1|\\mathbf b,\\sigma^2)}{f(\\mathbf y_2|\\mathbf b,\\sigma^2)}\\text{ is constant in }(\\mathbf b,\\sigma^2)\\quad\\Leftrightarrow\\quad(\\mathbf y_1^T\\mathbf y_1,\\mathbf X^T\\mathbf y_1)=(\\mathbf y_2^T\\mathbf y_2,\\mathbf X^T\\mathbf y_2) $$ First $$ \\frac{f(\\mathbf y_1|\\mathbf b,\\sigma^2)}{f(\\mathbf y_2|\\mathbf b,\\sigma^2)}=\\exp\\left\\{\\frac{1}{2\\sigma^2}(\\mathbf y_2^T\\mathbf y_2-\\mathbf y_1^T\\mathbf y_1)+\\frac{1}{\\sigma^2}\\mathbf b^T(\\mathbf X^T\\mathbf y_1-\\mathbf X^T\\mathbf y_2)\\right\\} $$ ($\\Leftarrow$) Obvious ($\\Rightarrow$) $\\forall (\\mathbf b,\\sigma^2)\\in\\R^p\\times\\R^+$, $\\frac{1}{2\\sigma^2}(\\mathbf y_2^T\\mathbf y_2-\\mathbf y_1^T\\mathbf y_1)+\\frac{1}{\\sigma^2}\\mathbf b^T(\\mathbf X^T\\mathbf y_1-\\mathbf X^T\\mathbf y_2)=c$ where $c$ does not depend on $(\\mathbf b,\\sigma^2)$. For fixed $\\sigma^2\u003e0$, $\\frac{1}{2}(\\mathbf y_2^T\\mathbf y_2-\\mathbf y_1^T\\mathbf y_1)-c\\sigma^2+\\mathbf b^T(\\mathbf X^T\\mathbf y_1-\\mathbf X^T\\mathbf y_2)=0$, $\\forall\\mathbf b\\in\\R^p$. By Result A.8, $\\mathbf X^T\\mathbf y_1-\\mathbf X^T\\mathbf y_2=\\mathbf 0$ $\\Rightarrow$ $\\frac{1}{2\\sigma^2}(\\mathbf y_2^T\\mathbf y_2-\\mathbf y_1^T\\mathbf y_1)=c$, $\\forall \\sigma^2\u003e0$ $\\Rightarrow$ $\\mathbf y_2^T\\mathbf y_2-\\mathbf y_1^T\\mathbf y_1=0$. Corollary 6.1: $\\mathbf y\\sim\\mathcal N_N(\\mathbf X\\mathbf b,\\sigma^2\\mathbf I)$. $(SSE,\\mathbf X^T\\mathbf y)$ is also minimal sufficient for $(\\mathbf b,\\sigma^2)$. Q: How does least squares related to M.L. (maximum likelihood)? A: Almost the same. Result 6.3: Assume $\\mathbf y\\sim\\mathcal N_N(\\mathbf X\\mathbf b,\\sigma^2\\mathbf I)$. Let $\\hat{\\mathbf b}$ be a solution to the N.E.s. $(\\hat{\\mathbf b},SSE/N)$ is a ML estimator of $(\\mathbf b,\\sigma^2)$. proof: Recall that $Q(\\mathbf b)=(\\mathbf y-\\mathbf X\\mathbf b)^T(\\mathbf y-\\mathbf X\\mathbf b)$. $$ L(\\mathbf b,\\sigma^2|\\mathbf y)=(2\\pi)^{-N/2}(\\sigma^2)^{-N/2}e^{-\\frac{1}{2\\sigma^2}Q(\\mathbf b)} $$ For any $\\sigma^2\u003e0$, $e^{-\\frac{1}{2\\sigma^2}Q(\\mathbf b)}$ is maximized by minimizing $Q(\\mathbf b)$. Of course, $Q(\\mathbf b)$ is minimized at $\\hat{\\mathbf b}$. So, we now can say for all $(\\mathbf b,\\sigma^2)\\in\\R^p\\times\\R^+$, $$ L(\\mathbf b,\\sigma^2|\\mathbf y)\\leq L(\\hat{\\mathbf b},\\sigma^2|\\mathbf y) $$ Now $$ \\log L(\\hat{\\mathbf b},\\sigma^2|\\mathbf y)=-\\frac{N}{2}\\log\\sigma^2-\\frac{1}{2\\sigma^2}Q(\\hat{\\mathbf b})+\\text{constant} $$ $$ \\frac{d}{d\\sigma^2}\\log L(\\hat{\\mathbf b},\\sigma^2|\\mathbf y)=-\\frac{N}{2\\sigma^2}+\\frac{1}{2(\\sigma^2)^2}Q(\\hat{\\mathbf b})\\stackrel{!}{=}0\\\\ \\Rightarrow\\hat{\\sigma^2}=\\frac{SSE}{N}\\\\ \\frac{d^2}{d(\\sigma^2)^2}\\log L(\\hat{\\mathbf b},\\sigma^2|\\mathbf y)\\bigg|_{\\hat{\\sigma^2}}\u003c0\\Rightarrow\\text{ Maximum} $$ Finally, we can say: for all $(\\mathbf b,\\sigma^2)\\in\\R^p\\times\\R^+$, $L(\\mathbf b,\\sigma^2|\\mathbf y)\\leq L(\\hat{\\mathbf b},\\sigma^2|\\mathbf y)\\leq L(\\hat{\\mathbf b},\\hat{\\sigma^2}|\\mathbf y)$. Corollary 6.3: Assume $\\mathbf y\\sim\\mathcal N_N(\\mathbf X\\mathbf b,\\sigma^2\\mathbf I)$. The ML estimator of an estimable function,","date":"2023-10-21","objectID":"/notes-for-sta6246-lm/:7:0","tags":null,"title":"STA6246: Theory of Linear Models","uri":"/notes-for-sta6246-lm/"},{"categories":null,"content":"This contains my notes for Statistical Inference","date":"2023-10-21","objectID":"/notes-for-sta7346-si/","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"Instructor: Prof. Zhihua Su Taken by Yu Zheng ","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:0:0","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"Chapter 1: Decision Theory, Bayesian Analysis ","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:1:0","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"1. General Non-sequential Decision Problem The general non-sequential decision theory consists of three basic elements. (a) A nonempty set of $\\Theta$ of possible states of nature, sometimes referred to as the parameter space; (b) A nonempty set of $\\mathcal A$ of actions available to the statistician; (c) A loss function $ L(\\theta, a)$, a map from $\\Theta\\times\\mathcal A$ to the reals. The triplet $(\\Theta,\\mathcal A,L)$ defines what is called a Game, and is now interpreted as follows. Nature chooses a point $\\theta$ in $\\Theta$, and the statistician without being informed of the choice of nature, chooses an action $a$ in $\\mathcal A$. As a consequence of these two choices, the statistician loses an amount $L(\\theta, a)$. Example 1: (Prisoner’s dilemma) Two prisoners were partners in a crime, and they were questioned in separate rooms. Each prisoner had a choice of confessing to the crime, and there by implicating the other, or denying that he had participated in the crime. If only one prisoner confessed, then he would go free, and the authorities would throw the book at the other prisoner, requiring him to spend 6 months in prison. If both prisoners denied being involved, then both would be held 1 month on a technicality, and if both prisoners confessed, they would both be held for 3 months. To create $(\\Theta,\\mathcal A,L)$ out of this, label prisoner 1 as nature, and prisoner 2 as statistician. Denoting by 1 and 2 the respective decision to confess or deny. One gets $\\Theta=\\{1,2\\}$, $\\mathcal A=\\{1,2\\}$. The loss function is now demonstrated in the following table: $\\mathcal A\\setminus\\Theta$ 1 2 1 3 0 2 6 1 That is, $L(1,1)=3,L(2,1)=0,L(1,2)=6,L(2,2)=1$. Example 2: Suppose $\\Theta=\\{\\theta_1=\\text{no rain},\\theta_2=\\text{light rain},\\theta_3=\\text{heavy rain}\\}$, $\\mathcal A=\\{a_1=\\text{carrying an umbrella},a_2=\\text{not carrying an umbrella}\\}$. $\\mathcal A\\setminus\\Theta$ $\\theta_1$ $\\theta_2$ $\\theta_3$ $a_1$ 1 -1 -4 $a_2$ 0 1 3 The above are examples of no-data decision problems. In statistics, however, we usually consider a random variable $X$ assuming values $x \\in X$ with a family of possible distributions $\\{P_\\theta,\\theta\\in\\Theta\\}$. If we observe $X = x$, we take the action $\\delta(x)\\in\\mathcal A$ according to some rule $\\delta$. Such a $\\delta$ is called a Non-randomized Decision Rule. Def: A non-randomized decision rule (function) $\\delta$ is a map from $\\mathcal X$ to $\\mathcal A$. If $X = x$ is observed, and $\\theta$ is the true state of nature, or true parameter, the loss incurred is $L(\\theta,\\delta(x))$. Note that $L(\\theta,\\delta(x))$ is a random variable. The average loss or risk is then given by $$ R(\\theta,\\delta)=\\mathbb E_\\theta L(\\theta,\\delta(x))=\\int L(\\theta,\\delta(x))f_\\theta(x)dx\\quad\\text{or }\\sum_xL(\\theta,\\delta(x))f_\\theta(x) $$ according as $f_\\theta(x)$ is the pdf or pmf of $X$. Let $\\mathcal D$ = class of all non-randomized decision rules $\\delta$ for which $R(\\theta,\\delta)$ is finite for all $\\theta\\in\\Theta$. Example 3: Consider the prisoners dilemma in Example 1. Suppose that before the prisoners are questioned, the statistician asked the nature if he was going to confess or deny. Assume that the nature answers truthfully with probability $3/4$. Then the statistician observes a random variable $X$ (the answer nature gives) taking the values $1$ or $2$. Note that $f_1(1)=f_2(2)=3/4$, $f_1(2)=f_2(1)=1/4$. There are four possible nonrandomized decision rules $\\delta_1,\\delta_2,\\delta_3,\\delta_4$ from $\\mathcal X={1,2}$ to $\\mathcal A={1,2}$. $$ \\delta_1(1)=\\delta_1(2)=1\\\\ \\delta_2(1)=1,\\delta_2(2)=2\\\\ \\delta_3(1)=2,\\delta_3(2)=1\\\\ \\delta_4(1)=\\delta_4(2)=2 $$ Rules $\\delta_1$ and $\\delta_4$ ignores the value of $X$. Rule $\\delta_2$ reflects the belief of the statistician that the nature is telling the truth, and rule $\\delta_3$ that the nature is not telling the truth. $$ R(\\theta,\\delta_1)=L(\\theta,\\delta_1(1))f_\\theta(1)+L(\\theta,\\delta_1(2))f_\\theta(2) $$ $$ R(1,\\delta_1)=L(1,","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:1:1","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"2. The Geometry of Bayes and Minimax Rules when the Parameter Space contains a Finite Number of Elements We first need to bring in the notion of Randomized Decision Rules. Suppose $\\mathcal D$ is the space of all non-randomized decision rules. We extend $\\mathcal D$ to $\\mathcal D^\\star$ where $\\mathcal D^\\star$ is the space of all probability distributions over $\\mathcal D$. For example, suppose $\\mathcal D=\\{\\delta_1,\\delta_2,\\delta_3,\\delta_4\\}$. A typical element of $\\mathcal D^\\star$ is a probability distribution $\\delta^\\star$ such that $\\delta^\\star$ assigns probability $p_i$ to $\\delta_i$ $(i=1,2,3,4)$, $p_i\\geq0$ and $\\sum_{i=1}^4p_i=1$. In general, we shall write the risk function corresponding to $\\delta^\\star$ as $R(\\theta,\\delta^\\star)=\\mathbb ER(\\theta,Y)$, where $Y$ is a random variable assuming values in $\\mathcal D$ given by $\\delta^\\star$. One advantage of the Bayes procedure is that the search for good decision rules may be restricted to the class of non-randomized decision rules. More precisely, if a Bayes rule with respect to a prior distribution $\\xi$ exists, there exists a non-randomized Bayes rule with respect to $\\xi$. A non-rigorous proof of this is as follows. proof: Suppose $\\delta_{\\xi}\\in\\mathcal D^\\star$ is a Bayes rule with respect to a prior distribution $\\xi$. Then $$ r(\\xi,\\delta_\\xi)=\\int_{\\Theta}R(\\theta,\\delta_\\xi)d\\xi(\\theta) $$ Let $Y$ denote the random variable assuming values in $\\mathcal D$ with distribution function given by $\\delta_\\xi$. Then $$ R(\\theta,\\delta_\\xi)=\\mathbb ER(\\theta, Y)=\\int_\\mathcal DR(\\theta,\\delta)d\\delta_\\xi(\\delta) $$ $$ \\begin{aligned} r(\\xi,\\delta_\\xi)\u0026=\\int_\\Theta\\int_\\mathcal DR(\\theta,\\delta)d\\delta_\\xi(\\delta)d\\xi(\\theta)\\\\ \u0026=\\int_\\mathcal D\\int_{\\Theta} R(\\theta,\\delta)d\\xi(\\theta)d\\delta_\\xi (\\delta)\\quad\\text{(assume that we can)}\\\\ \u0026=\\int_\\mathcal Dr(\\xi,\\delta)d\\delta_\\xi(\\delta)\\\\ \u0026=\\int_\\mathcal Dr(\\xi,\\delta_\\xi)d\\delta_\\xi(\\delta)+\\int_\\mathcal D\\left(r(\\xi,\\delta)-r(\\xi,\\delta_\\xi)\\right)d\\delta_\\xi(\\delta)\\\\ \u0026=r(\\xi,\\delta_\\xi)+\\int_\\mathcal D\\left(r(\\xi,\\delta)-r(\\xi,\\delta_\\xi)\\right)d\\delta_\\xi(\\delta) \\end{aligned} $$ Now $\\delta_\\xi$ is the Bayes rule w.r.t. $\\xi$, $r(\\xi,\\delta_\\xi)\\leq r(\\xi,\\delta)$ for every $\\delta\\in\\mathcal D$ (non-randomized). Thus $r(\\xi,\\delta)=r(\\xi,\\delta_\\xi)$ with probability 1. This implies that $\\delta$ is Bayes with respect to $\\xi$. ","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:1:2","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"3. Finding Bayes Rules First consider the no data problem. Suppose the loss involved in taking action $a$ when $\\theta$ is the true parameter is $L(\\theta,a)$. Let the prior pdf be given by $g(\\theta)$. Then the Bayes procedure consists in finding and $a_0$ (if possible) which minimizes $\\int L(\\theta,a)g(\\theta)d\\theta$ w.r.t. $a$. Suppose now $X$ is a random variable with pdf $f_\\theta(x)$. If $\\xi$ denote the prior distribution with pdf $g(\\theta)$, the Bayes risk of a decision rule $\\delta$ w.r.t. the prior $\\xi$ is given by $$ r(\\xi,\\delta)=\\int_\\Theta R(\\theta,\\delta)g(\\theta)d\\theta=\\int_\\Theta\\int_\\mathcal XL(\\theta,\\delta(x))f_\\theta(x)g(\\theta)dxd\\theta. $$ In this case, we interpret $f_\\theta(x)$ as the conditional pdf of $X$ given $\\theta$. Hence $f_\\theta(x)g(\\theta)=$ joint pdf of $X$ and $W$, where $W$ is a random variable assuming values $\\theta$ in $\\Theta$. An alternate way of writing $f_\\theta(x)g(\\theta)$ is $p(\\theta|x)h(x)$ where $p(\\theta|x)$ is the conditional pdf of $W$ given $X=x$, where $h(x)$ is the marginal pdf of $X$. To find a decision rule $\\delta_\\xi$ for which the Bayes risk is minimized, it suffices in finding a $\\delta_\\xi(x)$ (if possible) which minimizes $\\int_\\Theta L(\\theta, \\delta(x))p(\\theta|x)d\\theta$ for each $x\\in\\mathcal X$. If for each $x$, $\\delta_\\xi(x)$ is unique and $r(\\xi,\\delta_\\xi)$ is finite then we say the Bayes decision rule is unique. The problem of determining Bayes procedure arises in a number of contexts. (i) As a way of Utilizing Past Experience It is frequently reasonable to treat the parameter $\\theta$ of a statistical problem as the realization of a random variable $W$, rather than an unknown constant. Suppose, for example, that we wish to estimate the probability of a penny showing heads when spun on a flat surface. So far we would have considered $n$ spins of the penny as a set of $n$ binomial trials with an unknown probability p of showing heads. Suppose, however, we have had considerable experience with spinning pennies, which has provided us with approximate values of $p$ for a large number of similar experiments. if we believe this experience to be relevant to the present penny, it might be reasonable to represent past knowledge as a probability distribution for $p$, the approximate shape of which is suggested by the earlier data. This is not unlike the modeling which is usually done in statistics. An assumption such as the random variables representing the outcomes of our experiments have normal, Poisson, exponential distributions, and so on, we also draw on past experience. Futhermore, we also realize that these methods are in no sense exact, but at least represent reasonable approximations. There is the difference that in non-Bayesian parametric models, normally only the shape of the distribution is assumed to be known, but the values of the parameters are not assuming to be known. In a Bayesian analysis, however, the prior distribution is usually taken as completely specified. However, this is a difference in degree rather than in kind, and may be quite reasonable if the past experience is sufficiently extensive. A difficulty, of course, is the assumption that past experience is relevant to the present case. Perhaps, the mint has recently changed its manufacturing process, and the present coin, though similar in appearance as the earlier ones, has totally different spinning properties. But, a similar difficulty is associated with a non-Bayesian model. The choice of a prior distribution $\\xi(\\theta)$ is typically made by combining experience with conve- nience. When we assume that the amount of rainfall has a gamma distribution, we don’t do so because we really believe this to be the case, but because the gamma family is a two-parameter family which seems to fit such a prior distribution by starting with a flexible family that is math- ematically easy to handle and selecting a member from this family which approximates our past experience. Such an approa","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:1:3","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"4. Convex Loss and Non-Randomized Decision Rules Result: The function $f(x)=|x|^r$, $r\\geq 1$ is a convex function, where $|\\cdot|$ denote the Euclidean norm. proof: First show that $|\\alpha x+(1-\\alpha)y|\\leq\\alpha|x|+(1-\\alpha)|y|$. Note that $$ \\begin{aligned} \u0026|\\alpha x+(1-\\alpha)y|^2-\\left(\\alpha|x|+(1-\\alpha)|y|\\right)^2\\\\ =\u0026\\alpha^2|x|^2+(1-\\alpha)^2|y|^2+2\\alpha(1-\\alpha)\\langle x,y\\rangle-\\left[\\alpha^2|x|^2+(1-\\alpha)^2|y|^2+2\\alpha(1-\\alpha)|x||y|\\right]\\\\ =\u00262\\alpha(1-\\alpha)\\left(\\langle x,y\\rangle-|x||y|\\right)\\leq0 \\end{aligned} $$ Thus $|\\alpha x+(1-\\alpha)y|\\leq\\alpha|x|+(1-\\alpha)|y|$. When $r\\geq1$, it suffices to show that $\\left[\\alpha |x|+(1-\\alpha)|y|\\right]^r\\leq\\alpha|x|^r+(1-\\alpha)|y|^r$. Define a random variable $Z$ such that $P(Z=|x|)=\\alpha=1-P(Z=|y|)$. Then , we have $$ \\left[\\alpha |x|+(1-\\alpha)|y|\\right]^r=(\\mathbb EZ)^r\\\\ \\alpha|x|^r+(1-\\alpha)|y|^r=\\mathbb E(Z^r) $$ Since $Z^r$ is a convex function of $Z(\\geq0)$ for $r\\geq1$, applying Jensen’s inequality, one gets $\\mathbb E(Z^r)\\geq(\\mathbb EZ)^r$. Typically the loss function that we consider are of the form $L(\\theta,a)=|\\theta-a|^r(r\\geq1)$, which for fixed $\\theta$ are convex functions of $a$. Typically $r=2$ or $1$. Of course, one can view $L(\\theta,a)$ as a convex function of $\\theta$ for $a$ fixed due to the symmetry of the loss. It can be shown that for convex loss, or more specifically, for $L(\\theta,a)$ which are convex in $a$ for fixed $\\theta$, any randomized rule can be improved (in terms of risk) uniformly by a non-randomized rule. Hence, for such a loss, in particular, for squared error loss, we will only use non-randomized rules. (Justification: Let $\\delta^\\star$ be a randomized decision rule in $\\mathcal D^*$ for which $\\int |a|dF(\\delta)\u003c\\infty$, where $F(\\delta)$ is the distribution on $\\mathcal D^\\star$ that corresponds to $\\delta^\\star$. Let $\\delta(x)=\\int adF(\\delta)$. Then $L(\\theta,\\delta(x))\\leq\\int L(\\theta,a)dF(\\delta)=L(\\theta,\\delta^\\star(x))$.) For a convex loss, it is also possible to prove a generalized version of the Rao-Blackwell Theorem. Theorem (Rao-Blackwell): Let $W$ be any unbiased estimator of $r(\\theta)$, and let $T$ be a sufficient statistic for $\\theta$. Define $\\phi(T)=E(W|T)$. Then $\\mathbb E_\\theta\\phi(T)=r(\\theta)$ and $Var(\\phi(T))\\leq Var(W)$. Theorem: Let $\\gamma(\\theta)$ be an estimable parameter function. Ler $T$ be a complete sufficient statistic for $\\theta$. Then there exists a unique unbiased estimator of $\\gamma(\\theta)$ based on $T$, which has the smallest risk under any convex loss among all unbiased estimator of $\\gamma(\\theta)$. proof: Suppose $g(X)$ is the unbiased estimator of $\\gamma(\\theta)$ and let $h(T)=\\mathbb E(g(X)|T)$. Then for any convex loss $L(\\gamma(\\theta),a)$, convex in $a$, $$ \\begin{aligned} R(\\gamma(\\theta),g(X))\u0026=\\mathbb E(L(\\gamma(\\theta),g(X)))\\\\ \u0026=\\mathbb E\\mathbb E[L(\\gamma(\\theta),g(X))|T]\\\\ \u0026\\geq \\mathbb E[L(\\gamma(\\theta,\\mathbb E(g(X)|T)))]\\\\ \u0026=\\mathbb E[L(\\gamma(\\theta),h(T))]\\\\ \u0026=R(\\gamma(\\theta),h(T)). \\end{aligned} $$ Suppose we have $h_1(T)$ and $h_2(T)$. $\\mathbb E(h_1(T))=\\mathbb E(h_2(T))=\\gamma(\\theta)$, i.e., $\\mathbb E(h_1(T)-h_2(T))=0$. By completeness, $h_1(T)=h_2(T)$. ","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:1:4","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"5. Minimax Decision Rules Recall the definition of a minimax decision rule. The statistician wants to find a decision rule $\\delta^\\star_0$ which minimizes the worst that can happen to him, i.e., a $\\delta^\\star_0$ which minimizes $\\sup\\limits_{\\theta\\in\\Theta}R(\\theta,\\delta^\\star)$. For simplicity, assume the supremum is attained at $\\theta’\\in\\Theta$. Let $\\Xi$ be the class of all prior distribution on $\\Theta$. Then the following lemma holds. Lemma: $\\sup\\limits_{\\theta\\in\\Theta} R(\\theta,\\delta^\\star)=\\sup\\limits_{\\xi\\in\\Xi}\\int_\\Theta R(\\theta,\\delta^\\star)d\\xi(\\theta)\\stackrel{def}{=}\\sup\\limits_{\\xi\\in\\Xi} r(\\xi,\\delta^\\star)$. proof: $$ \\int_\\Theta R(\\theta,\\delta^\\star)d\\xi(\\theta)\\leq\\int_\\Theta \\sup_{\\theta\\in\\Theta}R(\\theta,\\delta^\\star)d\\xi(\\theta)=\\sup_{\\theta\\in\\Theta} R(\\theta,\\delta^\\star). $$ This implies $$ \\sup_{\\xi\\in\\Xi}r(\\xi,\\delta^\\star)=\\sup_{\\xi\\in\\Xi}\\int R(\\theta,\\delta^\\star)d\\xi(\\theta)\\leq\\sup_{\\theta\\in\\Theta} R(\\theta,\\delta^\\star)\\quad\\quad\\quad(1) $$ By assumption, $\\sup\\limits_{\\theta\\in\\Theta}R(\\theta,\\delta^\\star)=R(\\theta’,\\delta^\\star)$. Let $\\xi’$ denote a prior such that $\\xi’({\\theta’})=1$. Hence $$ R(\\theta’,\\delta^\\star)=\\int_\\Theta R(\\theta,\\delta^\\star)d\\xi’(\\theta)\\leq\\sup_{\\xi\\in\\Xi}\\int_\\Theta R(\\theta,\\delta^\\star)d\\xi(\\theta) $$ i.e. $$ \\sup_{\\theta\\in\\Theta}R(\\theta,\\delta^\\star)\\leq\\sup_{\\xi\\in\\Xi}r(\\xi,\\delta^\\star)\\quad\\quad\\quad(2) $$ Combining $(1)$ and $(2)$, the lemma follows. Thus a statistician’s aim is to find a $\\delta^\\star_0$ minimizing $\\sup\\limits_{\\xi\\in\\Xi}\\int_\\Theta R(\\theta,\\delta^\\star)d\\xi(\\theta)=\\sup\\limits_{\\xi\\in\\Xi}r(\\xi,\\delta^\\star)$. Conversely, nature’s aim is to choose a $\\xi$ which maximizes $\\inf\\limits_{\\delta^\\star\\in\\mathcal D^\\star}\\int_{\\theta\\in\\Theta}R(\\theta,\\delta^\\star)d\\xi(\\theta)=\\inf\\limits_{\\delta^\\star\\in\\mathcal D^*}r(\\xi,\\delta^\\star)$. Def: A distribution $\\xi_0\\in\\Xi$ is said to be least favorable if $$ \\inf_{\\delta^\\star\\in\\mathcal D^\\star}r(\\xi_0,\\delta^\\star)=\\sup_{\\xi\\in\\Xi} \\inf_{\\delta^*\\in\\mathcal D^\\star}r(\\xi,\\delta^\\star). $$ The value on the RHS is called the maximin or lower value of the game. The name “least favorable” derives from the fact that if the statistician were told which prior distribution nature was using, he would like least to be told a distribution $\\xi_0$ satistisfying the above definition. We may note that a least favorable distribution $\\xi_0$ does not necessarily exist. The fundamental theorem of game (minimax theorem) states that under certain assumption (Von Newmann (1928), Sion (1958) On general minimax theorem) $$ \\inf_{\\delta^*\\in\\mathcal D^\\star} \\sup_{\\xi\\in\\Xi}r(\\xi,\\delta^\\star)=\\sup_{\\xi\\in\\Xi} \\inf_{\\delta^\\star\\in\\mathcal D^\\star}r(\\xi,\\delta^\\star), $$ i.e., $$ \\inf_{\\delta^\\star\\in\\mathcal D^\\star} \\sup_{\\theta\\in\\Theta}R(\\theta,\\delta^\\star)=\\sup_{\\xi\\in\\Xi} \\inf_{\\delta^\\star\\in\\mathcal D^\\star}r(\\xi,\\delta^\\star). $$ Thus a Bayes rule corresponding to a least favorable prior is minimax. However, a least favorable prior need not necessarily exist, or even though it may exist, there is no direct way of putting hands on it. Hence, a general approach of finding minimax rules may not exist. We propose below a few techniques. Theorem (The Bayes method): Suppose that there is a distribution $\\xi$ over $\\Theta$ such that $r(\\xi,\\delta_\\xi)=\\int R(\\theta,\\delta_\\xi)d\\xi(\\theta)=\\sup\\limits_{\\theta\\in\\Theta}R(\\theta,\\delta_\\xi)$. Then $\\delta_\\xi$ is minimax If $\\delta_\\xi$ is unique Bayes w.r.t. $\\xi$, then it is the unique minimax procedure $\\xi$ is least favorable proof: Suppose $\\delta^\\star$ is a decision rule (randomized or non-randomized). Then $$ \\sup_{\\theta\\in\\Theta} R(\\theta,\\delta_\\xi)=\\int_\\Theta R(\\theta,\\delta_\\xi)d\\xi(\\theta)\\leq\\int_\\Theta R(\\theta,\\delta^\\star)d\\xi(\\theta)\\leq\\sup_{\\theta\\in\\Theta}R(\\theta,\\delta^\\star). $$ Hence, $\\delta_\\xi$ is minimax. The proof follows by replacing $\\leq$ by $\u003c$ in the first inequality in the first inequality above. Let $\\xi^\\sta","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:1:5","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"6. Admissibility Def: A decision rule $\\delta_1$ is said to be at least as good as $\\delta_2$ if $R(\\theta,\\delta_1)\\leq R(\\theta,\\delta_2)$ for all $\\theta\\in\\Theta$; $\\delta_1$ is said to be better than $\\delta_2$ if $R(\\theta,\\delta_1)\\leq R(\\theta,\\delta_2)$ for all $\\theta\\in\\Theta$ with strict inequality for some $\\theta\\in\\Theta$; $\\delta_1$ is said to be risk equivalent to $\\delta_2$ if $R(\\theta,\\delta_1)=R(\\theta,\\delta_2)$ for all $\\theta\\in\\Theta$. Def: A decision rule $\\delta_0$ is said to be admissible if there does not exist any decision rule $\\delta$ such that $R(\\theta,\\delta)\\leq R(\\theta,\\delta_0)$ for all $\\theta\\in\\Theta$ with strict inequality for some $\\theta\\in\\Theta$, i.e., there does not exist any rule better than $\\delta_0$. *Better than – Dominate; Admissible – No rule dominating it Def: A class $\\mathcal C(\\subset\\mathcal D^\\star)$ of decision rules is said to be complete if given any $\\delta\\in\\mathcal D^\\star$ such that $\\delta\\not\\in\\mathcal C$, there exists a rule $\\delta_0\\in\\mathcal C$ which is better than $\\delta$. A class $\\mathcal C(\\subset\\mathcal D^\\star)$ of decision rules is said to be essentially complete if given any rule $\\delta\\in\\mathcal D^\\star$ such that $\\delta\\not\\in\\mathcal C$, there exists a rule $\\delta_0\\in\\mathcal C$ which is at least as good as $\\delta$ The difference between complete and essentially complete class of decision rules may be illuminated by the following two lemmas. Lemma 1: If $\\mathcal C$ is a complete class, and $\\mathcal A$ denotes the class of admissible rules, then $\\mathcal A\\subset\\mathcal C$. proof: Suppose $\\mathcal A\\not\\subset\\mathcal C$. Then there exists $\\delta_0\\in\\mathcal A$, but $\\delta_0\\not\\in\\mathcal C$. Here, there is a $\\delta\\in\\mathcal C$ which is better than $\\delta_0$ contradicting the admissibility of $\\delta_0$. Lemma 2: If $\\mathcal C$ is an essentially complete class, and there exists an admissible $\\delta\\not\\in\\mathcal C$, then there exists a $\\delta’\\in\\mathcal C$ which is risk equivalent to $\\delta$. proof: Since $\\mathcal C$ is essentially complete, and $\\delta\\not\\in\\mathcal C$, there is a $\\delta’\\in\\mathcal C$ such that $R(\\theta,\\delta’)\\leq R(\\theta,\\delta)$ for all $\\theta\\in\\Theta$. Since $\\delta$ is admissible, strict inequality cannot occur for any $\\theta\\in\\Theta$. Hence, $R(\\theta,\\delta’)=R(\\theta,\\delta)$ for all $\\theta\\in\\Theta$. Def: A class $\\mathcal C$ of decision rules is said to be minimal complete if $\\mathcal C$ is complete, and no proper subclass of $\\mathcal C$ is complete. A class $\\mathcal C$ of decision rules is said to be minimal essentially complete if $\\mathcal C$ is essentially complete and no proper subclass of $\\mathcal C$ is essentially complete. *Remark: It is not necessary that minimal complete or minimal essentially complete classes exist. If the statistician can find an essentially complete class, there is no need for him to look outside this class to find a decision rule, for he can do just as well inside the class. Thus, if the statistician can find a small (essentially) complete class from which to make his choice, his task is greatly simplified. The smallest class may not exist, but if it does, it is called a minimal (essentially) complete class. The following clarifies the relationship between admissible rules and minimal complete classes. Theorem: If a minimal complete class exists, it consists exactly of the (all) admissible rules proof: Let $\\mathcal C$ denote the minimal complete class, and $\\mathcal A$ denote the class of all admissible rules. We need to show $\\mathcal C=\\mathcal A$. We have already proved that $\\mathcal A\\subset\\mathcal C$ since a minimal complete class is complete. So we need to show $\\mathcal C\\subset\\mathcal A$. Suppose there exists a $\\delta_0\\in\\mathcal C$, but $\\delta_0\\not\\in\\mathcal A$. Thus Step 1: There exists a $\\delta_1\\in\\mathcal C$ which is better than $\\delta_0$. Step 2: Show $\\mathcal C_1=\\mathcal C-{\\delta_0}$ is complete (which contradict","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:1:6","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"Chapter 2: Multiparameter Estimation ","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:2:0","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"Chapter 3: Asymptotic Theory of Estimation","date":"2023-10-21","objectID":"/notes-for-sta7346-si/:3:0","tags":null,"title":"STA7346: Statistical Inference","uri":"/notes-for-sta7346-si/"},{"categories":null,"content":"Preprints Yu Zheng and Leo L. Duan. Gibbs Sampling using Anti-correlation Gaussian Data Augmentation, with Applications to L1-ball-type Models. arxiv ","date":"2023-10-21","objectID":"/publications/:0:1","tags":null,"title":"Publications","uri":"/publications/"},{"categories":null,"content":"I am currenyly a Ph.D. candidate in Statistics at University of Florida (UF), advised by professor Leo L. Duan. My research interests mainly lie in Bayesian modeling and machine learning methods. I have proposed innovative models and algorithms for problems such as high-dimensional clustering, variable selection, and image smoothing. On top of that, I am also interested in the convergence theory of MCMC algorithms. ","date":"2023-10-21","objectID":"/biography/:0:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Research Interests Bayesian modeling High-dimensional problems Latent variable models Convergence theory of MCMC algorithms ","date":"2023-10-21","objectID":"/biography/:0:1","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Education Ph.D. in Statistics (Ongoing), 2021-2025 University of Florida B.S. in Mathematics and Applied Mathematics, 2016-2020 University of Science and Technology of China ","date":"2023-10-21","objectID":"/biography/:0:2","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"My CV ","date":"2023-10-21","objectID":"/cv/:0:0","tags":null,"title":"","uri":"/cv/"},{"categories":null,"content":"Course information: Syllabus ","date":"2023-05-21","objectID":"/teaching/sta3024/:0:1","tags":null,"title":"[Instructor] STA 3024: Introduction to Statistics II","uri":"/teaching/sta3024/"},{"categories":null,"content":"Lecture notes: (Credit to Maria Ripol for all the help in building the course structure and materials) week1 week2 week3 Mon \u0026 Wed, week3 Fri week4 Mon \u0026 Wed, week4 Fri week5 Mon \u0026 Wed week6 Fri week7 Mon \u0026 Wed, week7 Fri week8 Mon, week8 Fri week9 Mon week10 Mon, week10 Wed, week10 Fri week11 Mon, week11 Wed, week11 Fri week12 Mon, week12 Wed, week12 Fri week13 Mon, week13 Wed, week13 Fri week14 Mon, week14 Wed ","date":"2023-05-21","objectID":"/teaching/sta3024/:0:2","tags":null,"title":"[Instructor] STA 3024: Introduction to Statistics II","uri":"/teaching/sta3024/"},{"categories":null,"content":"Exams Exam 1 practice exam, Exam 1 version A Exam 2 practice exam, Exam 2 version A Exam 3 practice exam, Exam 3 version A ","date":"2023-05-21","objectID":"/teaching/sta3024/:0:3","tags":null,"title":"[Instructor] STA 3024: Introduction to Statistics II","uri":"/teaching/sta3024/"},{"categories":null,"content":"Syllabus ","date":"2022-09-01","objectID":"/teaching/sta6275/:0:0","tags":null,"title":"[Teaching Assistant] STA6275: Computing I: Optimization","uri":"/teaching/sta6275/"},{"categories":null,"content":"Instructor: Demetris Athienitis ","date":"2022-07-01","objectID":"/teaching/sta6166b/:0:0","tags":null,"title":"[Teaching Assistant] STA6166: Statistical Method in Research I","uri":"/teaching/sta6166b/"},{"categories":null,"content":"Instructor: Mak Xavier Syllabus ","date":"2022-05-01","objectID":"/teaching/sta3032b/:0:0","tags":null,"title":"[Teaching Assistant] STA3032: Engineer Statistics","uri":"/teaching/sta3032b/"},{"categories":null,"content":"Instructor: Linda Davis Syllabus ","date":"2022-02-01","objectID":"/teaching/sta3032a/:0:0","tags":null,"title":"[Teaching Assistant] STA3032: Engineer Statistics","uri":"/teaching/sta3032a/"},{"categories":null,"content":"Instructor: Brenda Betancourt ","date":"2021-09-01","objectID":"/teaching/sta6166a/:0:0","tags":null,"title":"[Teaching Assistant] STA6166: Statistical Method in Research I","uri":"/teaching/sta6166a/"},{"categories":null,"content":"Instructor: Huting Wang The course material highly resembles the one for Mathematical Analysis B2. ","date":"2020-02-01","objectID":"/teaching/mc/:0:0","tags":null,"title":"[Undergraduate Teaching Assistant] Multivariate Calculus","uri":"/teaching/mc/"},{"categories":null,"content":" Midterm Review Final Review ","date":"2019-09-01","objectID":"/teaching/ms/:0:0","tags":null,"title":"[Undergraduate Teaching Assistant] Mathematical Statistics","uri":"/teaching/ms/"},{"categories":null,"content":"Textbook Click here for the textbook. It is under revision every year. Please refer to the department of Mathematics at USTC for the lastest version. ","date":"2019-02-01","objectID":"/teaching/maii/:0:1","tags":null,"title":"[Undergraduate Teaching Assistant] Mathematical Analysis B2","uri":"/teaching/maii/"},{"categories":null,"content":"Below are some of the notes for the tutorial sessions: Week1-2 Week3-4 Week5-6 Week7-8 Midterm Review Week9-11 Week13-15 Final Review (part 1) Final Review (part 2) Credit to Yunfan Chang for notes week1-2, week7-8 and week13-15. Credit to Qianwei Xia for notes week5-6. ","date":"2019-02-01","objectID":"/teaching/maii/:0:2","tags":null,"title":"[Undergraduate Teaching Assistant] Mathematical Analysis B2","uri":"/teaching/maii/"},{"categories":null,"content":"Eexams Midterm exam with solutions Final exam with solutions Click here for some of the former exams. They are collected and integrated by Tian Wu. ","date":"2019-02-01","objectID":"/teaching/maii/:0:3","tags":null,"title":"[Undergraduate Teaching Assistant] Mathematical Analysis B2","uri":"/teaching/maii/"},{"categories":null,"content":"Textbook Click here for the textbook. It is under revision every year. Please refer to the department of Mathematics at USTC for the lastest version. ","date":"2018-09-01","objectID":"/teaching/mai/:0:1","tags":null,"title":"[Undergraduate Teaching Assistant] Mathematical Analysis B1","uri":"/teaching/mai/"},{"categories":null,"content":"Below are some of the notes for the tutorial sessions: Week1-2 Week3-4 Week5-6 Week7-8 Week9-10 Week11-12 Week13-14 Week15-16 Credit to Yunfan Chang for notes week5-6, week11-12 and week15-16. ","date":"2018-09-01","objectID":"/teaching/mai/:0:2","tags":null,"title":"[Undergraduate Teaching Assistant] Mathematical Analysis B1","uri":"/teaching/mai/"},{"categories":null,"content":"Eexams Midterm exam with solutions Final exam with solutions Click here for some of the former exams. They are collected and integrated by Tian Wu. ","date":"2018-09-01","objectID":"/teaching/mai/:0:3","tags":null,"title":"[Undergraduate Teaching Assistant] Mathematical Analysis B1","uri":"/teaching/mai/"}]